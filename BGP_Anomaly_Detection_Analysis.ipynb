{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttPI_MBPjDWW"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Imports and Setup\n",
        "# ==============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"TensorFlow Version:\", tf.__version__)\n",
        "\n",
        "# Set a consistent style for plots\n",
        "sns.set(style='whitegrid')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Data Loading and Preparation (CORRECTED)\n",
        "# ==============================================================================\n",
        "# Define the file paths\n",
        "DATA_PATH = 'BGP_RIPE_datasets_for_anomaly_detection_csv_revised_19022021/'\n",
        "\n",
        "# --- Load the BENIGN (normal) data ---\n",
        "# We add header=None to tell pandas to use default integer column names.\n",
        "print(\"Loading benign (normal) data...\")\n",
        "ripe_df = pd.read_csv(DATA_PATH + 'RIPE_regular.csv', header=None)\n",
        "bcnet_df = pd.read_csv(DATA_PATH + 'BCNET_regular.csv', header=None)\n",
        "normal_df = pd.concat([ripe_df, bcnet_df], ignore_index=True)\n",
        "\n",
        "# --- Extract the 37 features for training ---\n",
        "# Columns 4 to 40 will now have integer names. .iloc works on position, so it's correct.\n",
        "normal_features = normal_df.iloc[:, 4:41]\n",
        "print(f\"Loaded {len(normal_features)} samples of normal data.\")\n",
        "\n",
        "# --- Load the ANOMALY (attack) data ---\n",
        "print(\"\\nLoading anomaly (attack) data...\")\n",
        "anomaly_files = [\n",
        "    'WannaCrypt.csv', 'Moscow_blackout.csv', 'Slammer.csv', 'Nimda.csv', 'Code_Red_I.csv'\n",
        "]\n",
        "anomaly_dfs = {}\n",
        "for file in anomaly_files:\n",
        "    # Add header=None here as well for consistency.\n",
        "    anomaly_dfs[file.split('.')[0]] = pd.read_csv(DATA_PATH + file, header=None)\n",
        "    print(f\"- Loaded {file}: {len(anomaly_dfs[file.split('.')[0]])} samples\")"
      ],
      "metadata": {
        "id": "7by5O415lOvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Data Scaling and Chronological Splitting\n",
        "# ==============================================================================\n",
        "# We must split the data chronologically BEFORE scaling to prevent data leakage.\n",
        "# We will only train our model on normal data.\n",
        "\n",
        "# Chronological split of the normal data\n",
        "train_size = int(len(normal_features) * 0.8)\n",
        "train_df = normal_features[:train_size]\n",
        "val_df = normal_features[train_size:] # This part will be used to set the threshold\n",
        "\n",
        "print(f\"Training data size: {len(train_df)}\")\n",
        "print(f\"Validation data size (for thresholding): {len(val_df)}\")\n",
        "\n",
        "# --- Scaling ---\n",
        "# Initialize the scaler and fit it ONLY on the training data.\n",
        "scaler = StandardScaler()\n",
        "scaler = scaler.fit(train_df)\n",
        "\n",
        "# Apply the scaler to our training and validation sets\n",
        "train_scaled = scaler.transform(train_df)\n",
        "val_scaled = scaler.transform(val_df)"
      ],
      "metadata": {
        "id": "8btcGFPblsyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Sequence Creation Function\n",
        "# ==============================================================================\n",
        "# This function converts our 2D data into 3D sequences for the LSTM.\n",
        "def create_sequences(data, time_steps=60):\n",
        "    sequences = []\n",
        "    for i in range(len(data) - time_steps + 1):\n",
        "        sequences.append(data[i:i + time_steps])\n",
        "    return np.array(sequences)\n",
        "\n",
        "# Define the lookback window (how many seconds of data the model sees at once)\n",
        "TIME_STEPS = 60\n",
        "\n",
        "# Create the sequences for training and validation\n",
        "X_train_seq = create_sequences(train_scaled, TIME_STEPS)\n",
        "X_val_seq = create_sequences(val_scaled, TIME_STEPS)\n",
        "\n",
        "print(f\"Training sequences shape: {X_train_seq.shape}\")\n",
        "print(f\"Validation sequences shape: {X_val_seq.shape}\")"
      ],
      "metadata": {
        "id": "RAgUGka7lvew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Build the LSTM Autoencoder Model\n",
        "# ==============================================================================\n",
        "# Get the number of features from our data\n",
        "n_features = X_train_seq.shape[2]\n",
        "\n",
        "# Define the model architecture as described in the paper methodology\n",
        "model = keras.Sequential([\n",
        "    # Encoder\n",
        "    keras.layers.Input(shape=(TIME_STEPS, n_features)),\n",
        "    keras.layers.LSTM(128, activation='relu', return_sequences=True),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.LSTM(64, activation='relu', return_sequences=False),\n",
        "\n",
        "    # Bridge\n",
        "    keras.layers.RepeatVector(TIME_STEPS),\n",
        "\n",
        "    # Decoder\n",
        "    keras.layers.LSTM(64, activation='relu', return_sequences=True),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.LSTM(128, activation='relu', return_sequences=True),\n",
        "\n",
        "    # Output Layer\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(n_features))\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mae') # Mean Absolute Error is a good loss for reconstruction\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "UuiUqyRPlxmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Train the Model\n",
        "# ==============================================================================\n",
        "# Train the model to reconstruct normal sequences.\n",
        "# We use a callback to stop training early if the model stops improving.\n",
        "history = model.fit(\n",
        "    X_train_seq, X_train_seq,  # Input and target are the same for an autoencoder\n",
        "    epochs=30,                # A reasonable number of epochs\n",
        "    batch_size=64,            # Adjust based on memory\n",
        "    validation_data=(X_val_seq, X_val_seq),\n",
        "    callbacks=[\n",
        "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode='min', restore_best_weights=True)\n",
        "    ],\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "45z3tsG3l1Dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Determine the Anomaly Threshold (IMPROVED VERSION)\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Part 1: Visualize the training loss (This part is unchanged) ---\n",
        "plt.figure(figsize=(10, 5)) # Use a slightly wider figure for better readability\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Training History')\n",
        "plt.ylabel('Loss (Mean Absolute Error)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# --- Part 2: Calculate and visualize the threshold (This is the improved part) ---\n",
        "# Get the reconstruction error on the validation (normal) data\n",
        "print(\"\\nCalculating reconstruction errors on the validation set...\")\n",
        "X_val_pred = model.predict(X_val_seq)\n",
        "val_mae_loss = np.mean(np.abs(X_val_pred - X_val_seq), axis=(1, 2))\n",
        "\n",
        "# Plot the distribution of reconstruction errors with annotations\n",
        "print(\"Plotting the distribution of errors to determine threshold...\")\n",
        "plt.figure(figsize=(12, 6)) # A larger figure for the main plot\n",
        "sns.histplot(val_mae_loss, bins=50, kde=True, stat=\"density\")\n",
        "plt.title('Distribution of Reconstruction Error on Normal Data (Validation Set)')\n",
        "plt.xlabel('Mean Absolute Error (Reconstruction Error)')\n",
        "plt.ylabel('Density')\n",
        "\n",
        "# --- NEW: Add the threshold line and text to the plot ---\n",
        "# Set the anomaly threshold at the 99th percentile of the validation loss\n",
        "threshold = np.percentile(val_mae_loss, 99)\n",
        "print(f\"Anomaly Threshold (99th percentile) set to: {threshold:.4f}\")\n",
        "\n",
        "# Plot the threshold as a vertical red dashed line\n",
        "plt.axvline(threshold, color='r', linestyle='--',\n",
        "            label=f'99th Percentile Threshold = {threshold:.4f}')\n",
        "\n",
        "# Add a legend to show what the red line means\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XgmPNsc3mGTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Final Evaluation on Anomaly and Normal Data (CORRECTED & More Robust)\n",
        "# ==============================================================================\n",
        "\n",
        "def evaluate_model(data, labels):\n",
        "    \"\"\"A helper function to scale, sequence, and evaluate data.\"\"\"\n",
        "    data_scaled = scaler.transform(data)\n",
        "    sequences = create_sequences(data_scaled, TIME_STEPS)\n",
        "    sequence_labels = labels[TIME_STEPS - 1:].to_numpy() # Convert to numpy array for clean indexing\n",
        "\n",
        "    pred_sequences = model.predict(sequences)\n",
        "    test_mae_loss = np.mean(np.abs(pred_sequences - sequences), axis=(1, 2))\n",
        "\n",
        "    predicted_labels = (test_mae_loss > threshold).astype(int)\n",
        "\n",
        "    return sequence_labels, predicted_labels\n",
        "\n",
        "# --- Prepare the final, combined test set ---\n",
        "# Start with the unseen normal data (the validation set)\n",
        "# val_df already has the correct features (columns 4-40)\n",
        "test_normal_features = val_df.copy()\n",
        "# The label is 0 for all normal data.\n",
        "test_normal_features['label'] = 0\n",
        "\n",
        "# Now process and combine all anomaly files\n",
        "all_test_dfs = [test_normal_features]\n",
        "for name, df in anomaly_dfs.items():\n",
        "    # df has columns 0-41. We want features from 4-40 and the label from 41.\n",
        "    anomaly_features = df.iloc[:, 4:41].copy()\n",
        "    # The original label is in column 41. Let's make it 1 for anomaly.\n",
        "    anomaly_features['label'] = 1\n",
        "    all_test_dfs.append(anomaly_features)\n",
        "\n",
        "# Create one big test dataframe\n",
        "final_test_df = pd.concat(all_test_dfs, ignore_index=True)\n",
        "\n",
        "# Separate features and labels\n",
        "final_test_labels = final_test_df['label']\n",
        "# The feature columns are all columns EXCEPT the one named 'label'\n",
        "final_test_features = final_test_df.drop('label', axis=1)\n",
        "\n",
        "\n",
        "# --- Run the evaluation ---\n",
        "true_labels, predicted_labels = evaluate_model(final_test_features, final_test_labels)\n",
        "\n",
        "# --- Calculate and print metrics ---\n",
        "print(\"Final Evaluation Metrics:\")\n",
        "print(\"=\"*25)\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels)\n",
        "recall = recall_score(true_labels, predicted_labels)\n",
        "f1 = f1_score(true_labels, predicted_labels)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# --- Plot the confusion matrix ---\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Benign', 'Anomaly'], yticklabels=['Benign', 'Anomaly'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('Actual Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-6UaqsC2mJ6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Case Study - A Deeper Dive into a Specific Anomaly\n",
        "# ==============================================================================\n",
        "\n",
        "def run_case_study(anomaly_name, anomaly_df, scaler, model, threshold, time_steps):\n",
        "    \"\"\"\n",
        "    A function to evaluate and visualize results for a single anomaly dataset.\n",
        "\n",
        "    Args:\n",
        "        anomaly_name (str): The name of the anomaly for titles and labels.\n",
        "        anomaly_df (pd.DataFrame): The dataframe containing the anomaly data.\n",
        "        scaler (StandardScaler): The pre-fitted scaler object.\n",
        "        model (keras.Model): The trained LSTM autoencoder model.\n",
        "        threshold (float): The anomaly detection threshold.\n",
        "        time_steps (int): The sequence length.\n",
        "    \"\"\"\n",
        "    print(f\"--- Case Study: Evaluating {anomaly_name} Anomaly ---\")\n",
        "\n",
        "    # 1. Prepare data\n",
        "    features = anomaly_df.iloc[:, 4:41]\n",
        "    true_labels = np.ones(len(features)) # Ground truth is all anomalies (1)\n",
        "\n",
        "    # 2. Scale and sequence\n",
        "    scaled_features = scaler.transform(features)\n",
        "    sequences = create_sequences(scaled_features, time_steps)\n",
        "    true_labels_seq = true_labels[time_steps - 1:]\n",
        "\n",
        "    # 3. Get predictions and reconstruction error\n",
        "    print(f\"Predicting on {anomaly_name} sequences...\")\n",
        "    pred_sequences = model.predict(sequences)\n",
        "    mae_loss = np.mean(np.abs(pred_sequences - sequences), axis=(1, 2))\n",
        "\n",
        "    # 4. Classify and calculate recall\n",
        "    predicted_labels = (mae_loss > threshold).astype(int)\n",
        "    recall = recall_score(true_labels_seq, predicted_labels)\n",
        "\n",
        "    print(f\"\\nDetection Recall for {anomaly_name}: {recall:.4f}\")\n",
        "    print(f\"Detected {np.sum(predicted_labels)} out of {len(true_labels_seq)} anomalous sequences as anomalies.\")\n",
        "\n",
        "    # 5. Visualize the results\n",
        "    print(f\"\\nVisualizing reconstruction errors for {anomaly_name} vs. Normal Data...\")\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot the distribution of normal errors (val_mae_loss should be available from Cell 7)\n",
        "    sns.histplot(val_mae_loss, bins=50, kde=True, stat=\"density\", color=\"blue\", label='Normal Data Error')\n",
        "    # Plot the distribution of the current anomaly's errors\n",
        "    sns.histplot(mae_loss, bins=50, kde=True, stat=\"density\", color=\"red\", label=f'{anomaly_name} Data Error')\n",
        "    # Plot the threshold line\n",
        "    plt.axvline(threshold, color='black', linestyle='--', label=f'Anomaly Threshold = {threshold:.4f}')\n",
        "\n",
        "    plt.title(f'Comparison of Reconstruction Error: Normal vs. {anomaly_name}')\n",
        "    plt.xlabel('Mean Absolute Error (Reconstruction Error)')\n",
        "    plt.ylabel('Density')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# --- Now, let's run the case study for WannaCry ---\n",
        "# We retrieve the dataframe from the dictionary created in Cell 2.\n",
        "# Ensure Cell 2 has been run!\n",
        "wannacry_df_to_test = anomaly_dfs['WannaCrypt']\n",
        "run_case_study('WannaCry', wannacry_df_to_test, scaler, model, threshold, TIME_STEPS)\n",
        "\n",
        "# --- You can easily run it for another attack too! ---\n",
        "# slammer_df_to_test = anomaly_dfs['Slammer']\n",
        "# run_case_study('Slammer', slammer_df_to_test, scaler, model, threshold, TIME_STEPS)"
      ],
      "metadata": {
        "id": "cnx4cH9_p4EI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Case Study - Explicitly Testing the Moscow Blackout Anomaly\n",
        "# ==============================================================================\n",
        "\n",
        "# --- This section runs the case study specifically for the Moscow blackout ---\n",
        "# The 'run_case_study' function was defined in Cell 9. We are just calling it again\n",
        "# with a different dataset.\n",
        "\n",
        "# First, ensure the 'anomaly_dfs' dictionary is available from Cell 2.\n",
        "if 'anomaly_dfs' in locals() and 'Moscow_blackout' in anomaly_dfs:\n",
        "\n",
        "    # Retrieve the Moscow blackout dataframe from the dictionary\n",
        "    moscow_df_to_test = anomaly_dfs['Moscow_blackout']\n",
        "\n",
        "    # Call the main function to perform the evaluation and visualization\n",
        "    run_case_study(\n",
        "        anomaly_name='Moscow Blackout',\n",
        "        anomaly_df=moscow_df_to_test,\n",
        "        scaler=scaler,\n",
        "        model=model,\n",
        "        threshold=threshold,\n",
        "        time_steps=TIME_STEPS\n",
        "    )\n",
        "\n",
        "else:\n",
        "    print(\"Error: The 'anomaly_dfs' dictionary is not defined or does not contain 'Moscow_blackout'.\")\n",
        "    print(\"Please make sure you have run Cell 2 of the notebook successfully.\")"
      ],
      "metadata": {
        "id": "uDWbeJ9HrNgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Case Study - Explicitly Testing the Slammer Worm Anomaly\n",
        "# ==============================================================================\n",
        "\n",
        "# --- This section runs the case study specifically for the Slammer worm ---\n",
        "# We will reuse the 'run_case_study' function defined in Cell 9.\n",
        "\n",
        "# First, ensure the 'anomaly_dfs' dictionary is available from Cell 2.\n",
        "if 'anomaly_dfs' in locals() and 'Slammer' in anomaly_dfs:\n",
        "\n",
        "    # Retrieve the Slammer dataframe from the dictionary\n",
        "    slammer_df_to_test = anomaly_dfs['Slammer']\n",
        "\n",
        "    # Call the main function to perform the evaluation and visualization\n",
        "    run_case_study(\n",
        "        anomaly_name='Slammer Worm',\n",
        "        anomaly_df=slammer_df_to_test,\n",
        "        scaler=scaler,\n",
        "        model=model,\n",
        "        threshold=threshold,\n",
        "        time_steps=TIME_STEPS\n",
        "    )\n",
        "\n",
        "else:\n",
        "    print(\"Error: The 'anomaly_dfs' dictionary is not defined or does not contain 'Slammer'.\")\n",
        "    print(\"Please make sure you have run Cell 2 of the notebook successfully.\")"
      ],
      "metadata": {
        "id": "axWro-z1rr41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Generate a Synthetic \"BGP Storm\" Anomaly Dataset\n",
        "# ==============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "print(\"Generating a synthetic anomaly dataset: 'BGP_Storm_Anomaly.csv'\")\n",
        "\n",
        "# --- Define Parameters for our Synthetic Anomaly ---\n",
        "num_samples = 300  # A 5-minute anomaly event (300 seconds)\n",
        "num_features = 37\n",
        "\n",
        "# --- Create a base matrix of near-zero values ---\n",
        "# This simulates a baseline before the storm hits hard.\n",
        "data = np.random.uniform(low=0.0, high=0.1, size=(num_samples, num_features))\n",
        "\n",
        "# --- Inject the \"BGP Storm\" characteristics into key features ---\n",
        "# The storm will run for the middle 200 seconds of the event.\n",
        "start_index = 50\n",
        "end_index = 250\n",
        "storm_duration = end_index - start_index\n",
        "\n",
        "# Feature 1 (col 0): Number of announcements - Make it high and erratic\n",
        "data[start_index:end_index, 0] = np.random.randint(800, 1500, size=storm_duration)\n",
        "\n",
        "# Feature 2 (col 1): Number of withdrawals - Also high and erratic\n",
        "data[start_index:end_index, 1] = np.random.randint(800, 1500, size=storm_duration)\n",
        "\n",
        "# Feature 5 (col 4): Average AS-path length - Make it abnormally long\n",
        "data[start_index:end_index, 4] = np.random.randint(15, 25, size=storm_duration)\n",
        "\n",
        "# Feature 11 (col 10): Average edit distance - THIS IS A KEY INDICATOR OF CHAOS\n",
        "# Make it consistently high, showing constant path changes.\n",
        "data[start_index:end_index, 10] = np.random.uniform(8.0, 15.0, size=storm_duration)\n",
        "\n",
        "# Feature 12 (col 11): Maximum edit distance - Also very high\n",
        "data[start_index:end_index, 11] = data[start_index:end_index, 10] + np.random.randint(2, 5, size=storm_duration)\n",
        "\n",
        "# Feature 37 (col 36): Packet size - Make it large and variable due to long AS-paths\n",
        "data[start_index:end_index, 36] = np.random.randint(2000, 5000, size=storm_duration)\n",
        "\n",
        "\n",
        "# --- Assemble the final DataFrame in the correct format ---\n",
        "# Create a DataFrame from our numpy array\n",
        "features_df = pd.DataFrame(data)\n",
        "\n",
        "# Create dummy time columns (not used by the model, but needed for format consistency)\n",
        "time_df = pd.DataFrame(np.zeros((num_samples, 4)))\n",
        "\n",
        "# Create the label column - all are anomalies (label=1)\n",
        "labels_df = pd.DataFrame(np.ones((num_samples, 1)))\n",
        "\n",
        "# Concatenate all parts together\n",
        "final_synthetic_df = pd.concat([time_df, features_df, labels_df], axis=1)\n",
        "\n",
        "# --- Save to a CSV file ---\n",
        "# Save without a header and without the pandas index\n",
        "file_path = 'BGP_Storm_Anomaly.csv'\n",
        "final_synthetic_df.to_csv(file_path, header=False, index=False)\n",
        "\n",
        "print(f\"Successfully created '{file_path}' with {num_samples} samples.\")\n",
        "print(\"You can now upload this file or use it in the next cell to test the model.\")"
      ],
      "metadata": {
        "id": "8a4fdne3sYZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13: Case Study - Testing the Synthetic \"BGP Storm\" Anomaly\n",
        "# ==============================================================================\n",
        "\n",
        "# --- This section runs the case study on our generated anomaly data ---\n",
        "\n",
        "# Define the name and load the dataframe we just created\n",
        "anomaly_name_to_test = 'BGP Storm'\n",
        "try:\n",
        "    anomaly_df_to_test = pd.read_csv('BGP_Storm_Anomaly.csv', header=None)\n",
        "\n",
        "    # Call the main function to perform the evaluation and visualization\n",
        "    run_case_study(\n",
        "        anomaly_name=anomaly_name_to_test,\n",
        "        anomaly_df=anomaly_df_to_test,\n",
        "        scaler=scaler,\n",
        "        model=model,\n",
        "        threshold=threshold,\n",
        "        time_steps=TIME_STEPS\n",
        "    )\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'BGP_Storm_Anomaly.csv' not found.\")\n",
        "    print(\"Please make sure you have run the generation script in the previous cell successfully.\")"
      ],
      "metadata": {
        "id": "v7yoiC3vsaDa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}